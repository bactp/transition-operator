
## üîÅ Kubernetes Recovery Operator ‚Äì Logic Flow

```text
[ Start recovery loop ]
|
|-- Check CAPI Cluster: Ready?
|    |-- No: 
|         |-- Check InfraCluster readiness
|         |-- Check KubeadmControlPlane status
|         |-- Exit and requeue if not ready
|    |-- Yes: Continue
|
|-- List Machines (cluster.x-k8s.io/cluster-name = <cluster>)
|    |-- For each machine:
|        |-- Status != Running OR FailureReason set?
|            |-- Yes:
|                |-- If control plane and HA not met -> Skip
|                |-- Else -> Delete to trigger replacement
|
|-- List Nodes (corev1.Node)
|    |-- For each node:
|        |-- NotReady for more than N minutes?
|            |-- Yes: Cordon + Delete
|        |-- Has memory/disk/network pressure?
|            |-- Yes: Consider draining or re-allocating pods
|
|-- List Pods (all namespaces)
|    |-- For each pod:
|        |-- Status = Failed OR CrashLoopBackOff?
|            |-- Trace to owning controller:
|                |-- DaemonSet? Skip
|                |-- Deployment/ReplicaSet/StatefulSet?
|                    |-- If crash duration > threshold:
|                        |-- Delete pod for recreation
|                |-- No controller? Alert operator
|
|-- List Workloads (Deployments / StatefulSets / DaemonSets)
|    |-- For each workload:
|        |-- readyReplicas < replicas?
|            |-- Check rollout strategy / conditions
|            |-- If stalled > rollout timeout:
|                |-- Annotate or restart rollout
|
|-- Validate Critical CRDs present?
|    |-- No: Alert / reapply CRDs or reinstall operator
|
[ Repeat loop every N seconds based on ClusterPolicy ]
```
<!-- --- -->
<!-- [ Start recovery loop ]
|
|-- Check CAPI Cluster: Ready?
|    |-- No: Check if stuck due to infra or control plane
|    |-- Yes: Continue
|
|-- List Machines -> Check status
|    |-- Not running or failed -> delete to trigger re-creation
|
|-- List Nodes -> Check readiness and pressure conditions
|    |-- NotReady > N minutes -> cordon + delete
|
|-- List Pods (all namespaces)
|    |-- CrashLoopBackOff / Failed -> log and trace to parent
|        |-- If caused by bad node, mark node for recovery
|        |-- If caused by config, alert operator
|
|-- List Deployments/DaemonSets/StatefulSets
|    |-- .readyReplicas < .replicas -> investigate
|
[ Repeat ] -->


---
## A. Infra & Control-Plane Sanity
  ### A1. InfraCluster Ready?
  ```text
  infra := fetch InfraCluster for cluster
  if err or infra.Status.Ready != True:
    log error or ‚ÄúInfra not ready‚Äù
    annotate ClusterPolicy: ‚ÄúInfraUnready‚Äù
    return
  ```

  ### A2. ControlPlane Ready?
  ```text
  cp := fetch KubeadmControlPlane (or equivalent)
  if cp.Status.ReadyReplicas < cp.Spec.Replicas OR
     cp.HasCondition(‚ÄúInitialized‚Äù or ‚ÄúReady‚Äù, != True):
    log ‚ÄúControl plane not healthy‚Äù
    if age(cp) > policy.ControlPlaneTimeout:
      triggerControlPlaneRecovery(cp)
    return
  ```
## B. Machine Recovery
  ```text
  machines ‚Üê list Machines with label cluster-name=cluster.Name
  for each m in machines:
    if m.Status.Phase != ‚ÄúRunning‚Äù OR
       m.Status.FailureReason != nil:
      if isControlPlaneMachine(m) AND machines.ControlPlaneRunning < 2:
        # avoid single-CP outage
        log ‚ÄúDefer CP machine deletion until HA‚Äù
      else:
        delete m  # triggers MachineSet/MachineDeployment to recreate
        emit Metric ‚Äúmachine_deleted_total‚Äù
  ```

## C. Node Recovery
 ```text
  nodes ‚Üê list corev1.Node in all namespaces
  for each n in nodes:
    if !n.IsReady():
      if timeSince(n.LastTransition) > policy.NodeNotReadyThreshold:
        if hasBackingMachine(n):
          cordon n
          delete n
          emit Metric ‚Äúnode_replaced_total‚Äù
        else:
          log ‚ÄúNode without Machine‚Äîmanual intervention‚Äù
  ```

## D. Pod-Level Recovery
  ```text
  pods ‚Üê list corev1.Pod in all namespaces
  for each p in pods:
    if p.Status.Phase == ‚ÄúFailed‚Äù OR
       hasContainerState(p, Waiting, Reason=CrashLoopBackOff):
      owner := resolveController(p.OwnerReferences)
      if owner is DaemonSet:
        # DS pods auto-heal; just log
        log ‚ÄúDaemonSet pod crash‚Äù 
      else if owner is ReplicaSet/Deployment:
        # usually auto-healed; but‚Ä¶
        if timeSince(p.FirstObservedFailure) > policy.PodCrashThreshold:
          recordEvent(p, ‚ÄúPersistentCrashLoop‚Äù, ‚ÄúDeleting pod to force recreate‚Äù)
          delete p
      else:
        log ‚ÄúPod unmanaged or unknown owner‚Äîalert‚Äù
 ```
## E. Workload-Resource Health
  for each workload in {Deployment, StatefulSet, DaemonSet}:
  ```text
    wsList ‚Üê list workload in all namespaces
    for each ws in wsList:
      if ws.Status.ReadyReplicas < ws.Spec.Replicas:
        if timeSince(ws.Status.LastUpdateTime) > policy.WorkloadRolloutTimeout:
          annotate ws: ‚Äúrecovery.nephio.io/restarted=true‚Äù
          delete pods of ws  # force rollout retry
          emit Metric ‚Äúworkload_rollout_restarted_total‚Äù
   ```
## F. Loop & Backoff
  ## At end of each full pass:
  ```text
  if any recovery actions performed:
    requeue after policy.ShortInterval  # e.g. 30s
  else:
    requeue after policy.LongInterval   # e.g. 5m
  ```

## Create a secret with your Gitea credentials
```bash
kubectl create secret generic git-user-secret \
  --from-literal=username=nephio \
  --from-literal=password=secret \
  -n default
```
## Create environment variables
```bash
export GIT_SERVER_URL="http://192.168.28.137:30524"
export GIT_SECRET_NAME="git-user-secret"
export GIT_SECRET_NAMESPACE="default"
export POD_NAMESPACE="default"
```

```text
transition working - cilium CNI
```